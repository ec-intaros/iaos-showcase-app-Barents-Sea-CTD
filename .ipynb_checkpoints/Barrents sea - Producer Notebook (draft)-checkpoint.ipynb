{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Barrents sea - Producer Notebook\n",
    "\n",
    "## Introduction (TO EDIT)\n",
    "\n",
    "**Goal: generate a temperature map at a given depth, and at a given time, and within given geographical boundaries!**\n",
    "\n",
    "\n",
    "This Notebook provides guidance and support for data processing of CTD data, in particular by making use of the Geostatistical Library (RIntaros / RGeostats) for the modelisation of geostatistical relationships between variables, and to interpolate gridded data maps out of irregular CTD point measurements.\n",
    "\n",
    "The CTD data access point is configured for the OPeNDAP Hyrax server operated by the Institute of Marine Research, Norwegian Marine Data Centre, Norway\n",
    "http://opendap1.nodc.no/opendap/physics/point/yearly/contents.html\n",
    "\n",
    "Note: for this server, the **.nc4** extension from the data access URLs is not supported.\n",
    "\n",
    "The multi-year selection for CTD campaigns at sea is supported via the **year** parameter in the *fetch_data(url, year)* function in *helpers.py*, and consequently all the *fetch_data(url, year)* function calls within the Notebook. \n",
    "\n",
    "Citation: refer to the EC INTAROS project (https://cordis.europa.eu/project/id/727890)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# User-defined Input Parameters\n",
    "These are the required input parameters that the user must define for the analysis, keeping in mind the goal of this notebook to **generate a temperature map at a given depth, and at a given time, and within given geographical boundaries**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import modules\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pprint\n",
    "import xarray as xr\n",
    "import os\n",
    "\n",
    "from helpers import *\n",
    "\n",
    "from bokeh.plotting import figure, output_file, show, ColumnDataSource\n",
    "from bokeh.models import HoverTool\n",
    "from bokeh.io import output_notebook\n",
    "\n",
    "from datetime import datetime, timedelta\n",
    "import calendar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to apply margin for extensions\n",
    "margin = 20 # ie 20% of value\n",
    "def applyMargin(value, f):\n",
    "    \"\"\"\n",
    "    value: input value to apply the margin to\n",
    "    f: flag, must be \"low\" or \"high\" for lower or higher boundary\n",
    "    \"\"\"\n",
    "    if f == 'low': newvalue = value - (value * margin / 100)\n",
    "    elif f == 'high': newvalue = value + (value * margin / 100)\n",
    "    else: print('Wrong margin flag, must be \"low\" or \"high\".'); stop\n",
    "    return newvalue"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Depth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# User-defined Depth\n",
    "depth = 20\n",
    "\n",
    "# Apply margins\n",
    "depth1 = int(applyMargin(depth, f='low'))\n",
    "depth2 = int(applyMargin(depth, f='high'))\n",
    "\n",
    "# Display\n",
    "print('User-defined Depth:', depth)\n",
    "print(f'Depth range (including margins): {depth1} - {depth2}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bounding Box (BBOX)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# User-defined bbox label and boundaries\n",
    "bbox_key = 'UserAOI' # bbox label\n",
    "bbox_usr = [-5, 15, 53, 60] # bbox boundaries, in the format: minLong, maxLong, minLat, maxLat, eg [30, 60, 50, 80] \n",
    "\n",
    "# Apply margins\n",
    "bbox0 = int(applyMargin(bbox_usr[0], f='low'))\n",
    "bbox1 = int(applyMargin(bbox_usr[1], f='high'))\n",
    "bbox2 = int(applyMargin(bbox_usr[2], f='low'))\n",
    "bbox3 = int(applyMargin(bbox_usr[3], f='high'))\n",
    "bbox_values = [bbox0, bbox1, bbox2, bbox3]\n",
    "bbox_dict = {}\n",
    "bbox_dict[bbox_key] = bbox_values\n",
    "\n",
    "# Display\n",
    "print(f\"User-defined BBOX='{bbox_key}': {bbox_usr}\")\n",
    "print(f'BBOX range (including margins): {bbox_values}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Time Range"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# User-defined Month of interest start and end date in the format [day,month,year]\n",
    "year = 2003\n",
    "month = 2 # number of the month\n",
    "assert month > 1 and month < 12, 'Month must be between 2-11, to include margin of same calendar year'\n",
    "\n",
    "# Apply Margin\n",
    "month1 = month - 1\n",
    "month2 = month + 1\n",
    "\n",
    "day_start = 1\n",
    "day_end = calendar.monthrange(year, month2)[1] # only used for the last day of month\n",
    "\n",
    "time_start = datetime(year, month1, day_start)\n",
    "time_end = datetime(year, month2, day_end)\n",
    "\n",
    "# Display\n",
    "time_filter_str = f'{time_start.strftime(\"%Y-%m-%d\")} - {time_end.strftime(\"%Y-%m-%d\")}'\n",
    "print(f'User-defined Month: {datetime(year, month,1).strftime(\"%Y-%m\")}')\n",
    "print(f'Time range (including margins): {time_filter_str}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mesh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# User-define mesh (in degrees, equal for both long and lat)\n",
    "mesh = 1 \n",
    "\n",
    "# Display\n",
    "print(f'User-defined Mesh: {mesh} degree')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extraction of NetCDF Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exploratory Data Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": "true"
   },
   "source": [
    "### Set-up"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Server URL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define URL of Norwegian Marine Data Centre (NODC) data\n",
    "nodc_url = 'http://opendap1.nodc.no/opendap/physics/point/yearly/' # on NODC server\n",
    "\n",
    "server_url = nodc_url"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Define Year and Platform Codes \n",
    "The information on **year** and **platforms** available on the server (the individual NetCDF files) must be known a priori "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "platform_codes = ['AA', 'GS', 'GT', 'HJ', 'JH'] # Define codes of platforms of interest"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": "true"
   },
   "source": [
    "### Retrieval of DDS info\n",
    "Access data using the dds info, to retrieve the dimensions of the data for each dimensions: 'TIME', 'LATITUDE', 'LONGITUDE', 'DEPTH', 'POSITION'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get dds info, and assign max dimensions to TIME and DEPTH\n",
    "pc_dim_dict = {}\n",
    "time_stop_dict = {}\n",
    "depth_stop_dict = {}\n",
    "\n",
    "for pc in platform_codes:\n",
    "    \n",
    "    dds = f'{server_url}/58{pc}_CTD_{year}.nc.dds'\n",
    "        \n",
    "    pc_dim_dict[pc] = retrieveDDSinfo(dds)\n",
    "    \n",
    "    time_stop_dict[pc] = pc_dim_dict[pc]['TIME']\n",
    "    depth_stop_dict[pc] = pc_dim_dict[pc]['DEPTH']\n",
    "\n",
    "#pprint.pprint(pc_dim_dict)\n",
    "print('TIME:', time_stop_dict)\n",
    "print('DEPTH:', depth_stop_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visual Analysis: Load and Plot Positions only\n",
    "The objective of this section is to visualise the geograhical positions of the data for each platform, and to perform some filtering operations based on locations and time queries. This is possible using only the necessary information retrieved from the DDS. The key dimensions that are used for the position analysis are: 'TIME', 'LATITUDE', 'LONGITUDE'. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create *position_dict*\n",
    "This dictionary is used to store xarray data and attributes for the variables 'TIME', 'LATITUDE', 'LONGITUDE' of the point measurements of the available platforms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "position_dict = {}\n",
    "\n",
    "for pc in platform_codes:\n",
    "    \n",
    "    coords_str = getQueryString(pc_dim_dict[pc], keylist = ['TIME', 'LATITUDE', 'LONGITUDE']) # list the coordinates you want\n",
    "    \n",
    "    fix_lab = f'58{pc}_CTD_{year}' # label to use for this campaign\n",
    "    \n",
    "    url = f'{server_url}{fix_lab}.nc?{coords_str}'; print(f'Platform: {pc}. URL with Queries:', url)\n",
    "    \n",
    "    remote_data, data_attr = fetch_data(url, year)\n",
    "    \n",
    "    position_dict[pc] = {'data': remote_data, \n",
    "                         'data_attr': data_attr}\n",
    "    \n",
    "#     print(f'{data_attr}\\n')\n",
    "    \n",
    "# display(position_dict)\n",
    "\n",
    "# print(pc_dim_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Match and merge LAT, LONG and TIME of positions in a *position_df* dataframe\n",
    "Generate a pandas dataframe (*lonlat_df*) to store all locations (**LONGITUDE** and **LATITUDE**) and respective **TIME** for all platforms, in order to plot and visualise them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load locations (long & lat) and TIME of ALL measurements\n",
    "start_date = datetime(1950, 1, 1)\n",
    "    \n",
    "position_df = pd.DataFrame() \n",
    "\n",
    "for key in position_dict.keys():\n",
    "    test = pd.DataFrame()\n",
    "    \n",
    "    test['Longitude'] = position_dict[key]['data']['LONGITUDE'].data.astype(float)\n",
    "    test['Latitude'] = position_dict[key]['data']['LATITUDE'].data.astype(float)\n",
    "    test['Time'] = position_dict[key]['data']['TIME'].data.astype(float)\n",
    "    test['Platform'] = key\n",
    "    \n",
    "    # Convert TIME from float to datetime\n",
    "    test['Time'] = [start_date + timedelta(t) for t in test.loc[:,'Time']]\n",
    "    length = len(test[test['Platform']==key])\n",
    "    print(f'Platform {key}: {length} measurement locations.')\n",
    "    display(test.tail())\n",
    "    \n",
    "    position_df = position_df.append(test) \n",
    "    \n",
    "position_df['Index_ABS'] = np.arange(0,len(position_df))\n",
    "position_df = position_df.rename_axis(\"Index_Relative\")\n",
    "\n",
    "print(f'\\nMerged dataframe with all platforms. Total of {len(position_df)} measurement locations')\n",
    "position_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Plot Histogram of measurements of the given year"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "position_df.groupby(position_df[\"Time\"].dt.month).count().plot(kind=\"bar\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Plot Positions\n",
    "Plot locations on an interactive plot. Hoover your mouse on a location to see longitude and latitude information. All locations are showed in blue, and only filtered locations are shown in red."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plotIteractive(df2p, title, xlim, ylim, bbox=False):\n",
    "    output_notebook() # necessary to show the plot \n",
    "\n",
    "    hover = HoverTool(\n",
    "            tooltips=[\n",
    "                (\"index\", \"$index\"),\n",
    "                (\"(x,y)\", \"($x, $y)\"),\n",
    "                (\"Platform\", \"@Platform\"),            \n",
    "            ]\n",
    "        )\n",
    "\n",
    "    p = figure(plot_width=500, \n",
    "               plot_height=500, \n",
    "               tools=[hover],\n",
    "               title=title,\n",
    "               x_range=xlim, \n",
    "               y_range=ylim)\n",
    "\n",
    "    # p.square('Longitude', 'Latitude', size=6, color='grey', fill_color='white', source=df2p, legend_label=\"All locations\")\n",
    "    if any(df2p['Platform']=='AA'): p.circle('Longitude', 'Latitude', size=4, color='red', fill_color='white', source=df2p[df2p['Platform']=='AA'], legend_label=\"Platform AA\")\n",
    "    if any(df2p['Platform']=='GS'): p.circle('Longitude', 'Latitude', size=4, color='blue', fill_color='white', source=df2p[df2p['Platform']=='GS'], legend_label=\"Platform GS\")\n",
    "    if any(df2p['Platform']=='GT'): p.circle('Longitude', 'Latitude', size=4, color='green', fill_color='white', source=df2p[df2p['Platform']=='GT'], legend_label=\"Platform GT\")\n",
    "    if any(df2p['Platform']=='HJ'): p.circle('Longitude', 'Latitude', size=4, color='black', fill_color='white', source=df2p[df2p['Platform']=='HJ'], legend_label=\"Platform HJ\")\n",
    "    if any(df2p['Platform']=='JH'): p.circle('Longitude', 'Latitude', size=4, color='orange', fill_color='white', source=df2p[df2p['Platform']=='JH'], legend_label=\"Platform JH\")\n",
    "\n",
    "    # Add area\n",
    "    if bbox: \n",
    "        bbox_val = list(bbox_dict.values())[0]\n",
    "        p.quad(left=bbox_val[0], right=bbox_val[1], top=bbox_val[3], bottom=bbox_val[2], \n",
    "               legend_label=list(bbox_dict.keys())[0], fill_color='grey', fill_alpha=0.0, line_color=\"black\")\n",
    "\n",
    "    p.legend.location = \"bottom_right\"\n",
    "    show(p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xlim = (min(position_df['Longitude'])-1, max(position_df['Longitude'])+1)\n",
    "ylim = (min(position_df['Latitude'])-1, max(position_df['Latitude'])+1)\n",
    "\n",
    "title = f'All measurement points'\n",
    "plotIteractive(position_df, title, xlim, ylim, bbox=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Based on the plotting above, some decisions could be made on bounding box of interest, platform, etc. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Filter Positions (based on *position_df*)\n",
    "This section shows a few examples of data filtering by using the 'LATITUDE', 'LONGITUDE' and 'TIME' dimensions. First, the data is filtered by bounding box (BBOX). Subsequently, this data is further filtered using a time range."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Filter by BBOX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now remove the ouliers outside bbox\n",
    "position_df_bbox = position_df[(position_df.loc[:,'Longitude'] >= bbox_values[0]) & \n",
    "                               (position_df.loc[:,'Longitude'] <= bbox_values[1]) & \n",
    "                               (position_df.loc[:,'Latitude'] >= bbox_values[2]) & \n",
    "                               (position_df.loc[:,'Latitude'] <= bbox_values[3])]\n",
    "\n",
    "print(f'BBOX={bbox_key} range (including margins): {bbox_values}')\n",
    "sel_outof_all = f'{len(position_df_bbox)} out of {len(position_df)}.'\n",
    "print(f'Selected positions (out of available positions): {sel_outof_all}')\n",
    "\n",
    "display(position_df_bbox)\n",
    "\n",
    "title = f'Filtered data: BBOX={bbox_key}'\n",
    "plotIteractive(position_df_bbox, title, xlim, ylim, bbox_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Filter by BBOX, adding Time Range filter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "position_df_bbox_timerange = position_df_bbox.loc[(position_df_bbox['Time']>=time_start) & \n",
    "                                                  (position_df_bbox['Time']<=time_end)]\n",
    "\n",
    "print(f'BBOX={bbox_key} range (including margins): {bbox_values}')\n",
    "print(f'Time range (including margins): {time_filter_str}')\n",
    "sel_outof_all = f'{len(position_df_bbox_timerange)} out of {len(position_df)}.'\n",
    "print(f'Selected positions (out of available positions): {sel_outof_all}')\n",
    "display(position_df_bbox_timerange)\n",
    "\n",
    "title = f'Filtered data: BBOX={bbox_key} and Time={time_filter_str}'\n",
    "plotIteractive(position_df_bbox_timerange, title, xlim, ylim, bbox_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Print or Export Filtered Positions\n",
    "Uncomment the rows below if you want to display or export to CSV the filtered dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Print filtered dataframe\n",
    "# pd.set_option('display.max_rows', None)\n",
    "# display(position_df_bbox_timerange)\n",
    "\n",
    "# # Save dataframe to csv\n",
    "# data_dir = os.path.join(os.getcwd(), 'data_output')\n",
    "# if not os.path.exists(data_dir): os.mkdir(data_dir)\n",
    "# csvname = os.path.join(data_dir, f'filtered_{pc}_df.csv')\n",
    "# position_df_bbox_timerange.to_csv(csvname, sep=',', header=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Define Filtered Dataframe (*df_toPlot*) to be used for online querying of the filtered positions\n",
    "Define the filtered dataframe (eg *position_df_bbox*, *position_df_bbox_timerange*), to be named **df_toPlot**, and the dictionary of indices of filtered data (to be named **index_dict**), to use for further filtering."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define dataframe to plot based on one of the previously defined filters\n",
    "df_toPlot = position_df_bbox_timerange # or position_df / position_df_bbox\n",
    "\n",
    "sel_outof_all = f'{len(df_toPlot)} out of {len(position_df)}.'\n",
    "print(f'Filtered positions in \"BBOX={bbox_key}\" and \"Time range={time_filter_str}\" (out of available positions): {sel_outof_all}')\n",
    "\n",
    "display(df_toPlot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "index_dict = {}\n",
    "\n",
    "for pc in df_toPlot['Platform'].unique():\n",
    "    index_dict[pc] = df_toPlot[df_toPlot['Platform']==pc].index.tolist()\n",
    "\n",
    "# index_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Processing: Load and Plot selected Data (Variables within DEPTH range)\n",
    "\n",
    "This section enables accessing data of **only the selected variable** and **within a specified DEPTH range**, to avoid fetching unnecessary data and minimise data volume.\n",
    "\n",
    "The list of variables need to be specified in the *vars_sel* variable. The four variables that are available in this dataset are: \n",
    "* **PRES**: Sea Water Pressure\n",
    "* **TEMP**: Sea Water Temperature\n",
    "* **PSAL**: Sea Water Practical Salinity\n",
    "* **CNDC**: Sea Water Electrical Conductivity \n",
    "\n",
    "The depth range must also be defined. For some limitations to the DAP syntax, at least one range boundary needs to correspond to one of the two extremes. For example, in a data array of 100 elements starting from 0 to 99, the following scenarios are possible:\n",
    "* select the first 20 elements, corresponding to the values range 0 - 19 --> ```[first:1:intermediate] (eg [0:1:19])``` work\n",
    "* select the last 20 elements, corresponsing to the values range 80 - 99 --> ```[intermediate:1:last] (eg [80:1:99])``` work\n",
    "* select the intermediate 60-80 elements, corresponsing to the values range 60 - 79 --> ```[intermediate_1:1:intermediate_2] (eg [80:1:89])``` does NOT work"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Extraction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Available variables\n",
    "vars_main = ['PRES', 'TEMP', 'PSAL', 'CNDC'] \n",
    "\n",
    "# Define the selection of variable to use for the analysis\n",
    "vars_sel = ['TEMP', 'CNDC', 'PSAL']; assert all([elem in vars_main for elem in vars_sel])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create **data_dict** dictionary \n",
    "Once variables are defined, the DEPTH range must be defined. \n",
    "\n",
    "**INSERT BELOW** the desired DEPTH range (in meters) in the two fields of hte following cell, noting that: \n",
    "* ***metadata[pc]['depth_m_v1']***: either this is equal to the lower bound (ie index=0)\n",
    "* ***metadata[pc]['depth_m_v2']***: or is equal to the upper bound (ie index=-1)\n",
    "\n",
    "Once the variables and DEPTH range are defined, data and their attributes are read iteratively for each platform, and saved into a dictionary *data_dict* which contains:\n",
    "* the actual data, loaded into an **xarray** for data handling, analysis and visualisation\n",
    "* the campaign's main attributes: platform code & name, data type, title, instrument, longitude & latitude, and vertical min & max)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dict = {}\n",
    "metadata = {}\n",
    "\n",
    "for pc in df_toPlot['Platform'].unique():\n",
    "    \n",
    "    metadata[pc] = {}\n",
    "\n",
    "    v_min = int(float(position_dict[pc]['data_attr'][6]))\n",
    "    metadata[pc]['vmin'] = v_min\n",
    "    metadata[pc]['depth_abs_v1'] = 0 # this is fixed\n",
    "    metadata[pc]['depth_abs_v2'] = pc_dim_dict[pc]['DEPTH'] # this is fixed\n",
    "\n",
    "    # ==============================================================================\n",
    "    \"\"\"\n",
    "    Define here the DEPTH range of your selection, in meters. Note that either:\n",
    "    - 'depth_m_v1' is equal to the lower bound (ie index=0), or \n",
    "    - 'depth_m_v2' is equal to the upper bound (ie index=-1)\n",
    "    \"\"\"\n",
    "    metadata[pc]['depth_m_v1'] = 0\n",
    "    metadata[pc]['depth_m_v2'] = depth2 #pc_dim_dict[pc]['DEPTH'] \n",
    "    # ==============================================================================\n",
    "\n",
    "    # assert metadata[pc]['depth_m_v1'] < metadata[pc]['depth_m_v2'], 'ERROR: the lower bound must be lower than the higher bound' \n",
    "    # assert metadata[pc]['depth_m_v1'] == 0 or metadata[pc]['depth_m_v2'] == pc_dim_dict[pc]['DEPTH'], 'ERROR: one of the two values must be equal to one of the lower/upper bounds'\n",
    "\n",
    "    #     print(f'DEPTH range of interest (meters): {metadata[pc][\"depth_m_v1\"]} - {metadata[pc][\"depth_m_v2\"]}')\n",
    "\n",
    "    # the start and stop values are adjusted based on the vmin value\n",
    "    if metadata[pc]['vmin'] == 1: \n",
    "        if metadata[pc]['depth_m_v1'] == 0: # \n",
    "            metadata[pc]['depth_newindex_v1'] = metadata[pc]['depth_m_v1'] # the same\n",
    "            metadata[pc]['depth_newindex_v2'] = metadata[pc]['depth_m_v2'] # the same, so I have the right size. When I shift and add the nan, I get rid of further element on the right\n",
    "            metadata[pc]['depth_newindex4xr_v2'] = metadata[pc]['depth_m_v2']# - 1\n",
    "\n",
    "        elif metadata[pc]['depth_m_v1'] != 0: \n",
    "            metadata[pc]['depth_newindex_v1'] = metadata[pc]['depth_m_v1'] - 1 # start one element before\n",
    "            metadata[pc]['depth_newindex_v2'] = metadata[pc]['depth_m_v2'] - 1 # last element is excluded, ie stop one element before. But then I'll have to remoove one element\n",
    "            metadata[pc]['depth_newindex4xr_v2'] = metadata[pc]['depth_m_v2'] - metadata[pc]['depth_m_v1'] - 1 \n",
    "\n",
    "    else:\n",
    "        metadata[pc]['depth_newindex_v1'] = metadata[pc]['depth_m_v1']\n",
    "        metadata[pc]['depth_newindex_v2'] = metadata[pc]['depth_m_v2']\n",
    "\n",
    "        if metadata[pc]['depth_m_v1'] == 0: # \n",
    "            metadata[pc]['depth_newindex4xr_v2'] = metadata[pc]['depth_m_v2']\n",
    "\n",
    "        elif metadata[pc]['depth_m_v1'] != 0: \n",
    "            metadata[pc]['depth_newindex4xr_v2'] = metadata[pc]['depth_m_v2'] - metadata[pc]['depth_m_v1']\n",
    "\n",
    "    metadata[pc]['depth_newindex4xr_v1'] = 0\n",
    "\n",
    "    pprint.pprint(metadata[pc])\n",
    "    print(f'{pc} DEPTH range of interest (adjusted with vmin): {metadata[pc][\"depth_newindex_v1\"]} - {metadata[pc][\"depth_newindex_v2\"]}')\n",
    "\n",
    "    fix_lab = f'58{pc}_CTD_{year}' # platform_codes and year are defined at the beginning of the notebook \n",
    "\n",
    "    # Get coordinates (needed for keeping hte correct structure, and for plotting) \n",
    "    coords_str = getQueryString(pc_dim_dict[pc], keylist = ['TIME', 'LATITUDE', 'LONGITUDE']) # list the coordinates you want\n",
    "\n",
    "    # Extract TIME and DEPTH dimension for queries \n",
    "    time_dims = getQuery(pc, start=0, stop=pc_dim_dict[pc]['TIME'])\n",
    "    depth_dims = getQuery(pc, start=metadata[pc]['depth_newindex_v1'], stop=metadata[pc]['depth_newindex_v2'])#; print(depth_dims)\n",
    "\n",
    "    # join TIME and DEPTH for Variables\n",
    "    var_str_ALL = []\n",
    "    for v in vars_sel: var_str_ALL = np.append(var_str_ALL, f'{v}{time_dims}{depth_dims}')\n",
    "    queries_vars = ','.join(var_str_ALL)\n",
    "\n",
    "    # Build url and url with queries (url_q)\n",
    "    url = f'{server_url}{fix_lab}.nc?{coords_str}' \n",
    "    url_q = f'{url},{queries_vars}'; print(f'Platform {pc} URL:', url_q)\n",
    "\n",
    "    remote_data, data_attr = fetch_data(url_q, year)\n",
    "\n",
    "    data_dict[pc] = {'data': remote_data, \n",
    "                     'data_attr': data_attr}\n",
    "\n",
    "    print(f'{data_attr}\\n')\n",
    "\n",
    "# display(data_dict)\n",
    "print(f'Checking the existing campaigns in the dictionary: {list(data_dict.keys())}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Overview Dataframe with Platforms' Attributes\n",
    "An overview dataframe overview_df is then generated to show the detailed information about each campaign at sea: platform code & name, data type, title, instrument, longitude & latitude, and vertical min & max)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create database with the selected campaigns & years\n",
    "overview_df = pd.DataFrame()\n",
    "overview_df = getAttributes(overview_df, data_dict)\n",
    "overview_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract / define the variables to use for the analysis\n",
    "print('\\nPrinting DEPTH range for analyis:')\n",
    "assert len(np.unique([metadata[k][\"depth_m_v1\"] for k in metadata.keys()])==1)\n",
    "assert len(np.unique([metadata[k][\"depth_m_v2\"] for k in metadata.keys()])==1)\n",
    "\n",
    "for k in data_dict.keys():\n",
    "    print(f'{k}; Initial DEPTH range: {metadata[k][\"depth_m_v1\"]}-{metadata[k][\"depth_m_v2\"]}m; VARS: {list(data_dict[key][\"data\"].variables)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate vmin dictionary (needed to avoid doing the vmin adjustment more than once)\n",
    "vmin_dict = {}\n",
    "\n",
    "# select only those platforms where vmin == 1\n",
    "vmin_pc = overview_df[overview_df['Vertical_min'] == 1.0].index\n",
    "\n",
    "for i in vmin_pc:\n",
    "    vmin_dict[i] = {}\n",
    "    \n",
    "    for v in vars_sel:\n",
    "        vmin_dict[i][v] = False\n",
    "\n",
    "vmin_dict   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Filtering by DEPTH\n",
    "This section allows adding the filter based on DEPTH on the previously filtered data (based on BBOX and time range).\n",
    "\n",
    "The output of this operation is a *filtered_xarr* xarray dataset, containing one variable at the specified DEPTH range. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Data filtered by DEPTH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_xarr_dict = {}\n",
    "\n",
    "for pc in data_dict.keys():\n",
    "    \n",
    "    # Generate a filtered xarray with all variables for selected Platform, for a certain DEPTH range\n",
    "    if metadata[pc]['depth_m_v1']==0: align_and_nan = True\n",
    "    else: align_and_nan = False\n",
    "\n",
    "    for v in vars_sel: \n",
    "        check_alignment(data_dict, pc, v, align_and_nan, vmin_dict)\n",
    "\n",
    "    filtered_xarr_dict[pc] = filter_xarr_DEPTH(df_toPlot, \n",
    "                                               data_dict,\n",
    "                                               platform=pc,\n",
    "                                               depth_range=[depth1, depth2])\n",
    "display(filtered_xarr_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Aggregation of Available Platforms\n",
    "Two xarray datasets can be merged if they have the same structure, i.e. dimensions. First check the dimensions of DEPTH are the same."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dictionary of variables for each platform\n",
    "data_var_dict = {}\n",
    "depth_arr = []\n",
    "\n",
    "for pc in data_dict.keys():\n",
    "    \n",
    "    data_var_dict[pc] = {}\n",
    "    data = filtered_xarr_dict[pc]\n",
    "    \n",
    "    depth_dim_pc = data.dims[\"DEPTH\"]\n",
    "    depth_arr.append(depth_dim_pc)\n",
    "    \n",
    "    print(f'PC {pc}\\tFiltered Dims: TIME={data.dims[\"TIME\"]}, DEPTH={data.dims[\"DEPTH\"]}')\n",
    "    \n",
    "    for var in vars_sel:\n",
    "        data_var_dict[pc][var] = filtered_xarr_dict[pc][var]\n",
    "        \n",
    "assert all(x==depth_arr[0] for x in depth_arr), 'ERROR, the DEPTH dimensions must be equal.'\n",
    "# display(data_var_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now aggregate and plot each variable: on the y-axis is shown the TIME of the measurement (in float format, which needs to be converted to datetime format), and on the x-axis is the DEPTH of the measurement."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine arrays across platforms, for each variable\n",
    "merged_arr = {}\n",
    "\n",
    "for var in vars_sel:\n",
    "        \n",
    "    merged_arr[var] = xr.merge([data_var_dict[pc][var] for pc in data_dict.keys()])  \n",
    "        \n",
    "    title = f'Var={var} (Merged Platforms)\\nFilter: Time Range={time_filter_str};\\nBBOX={bbox_key}; Depth Range {depth1}-{depth2}m;\\nSel/All={sel_outof_all}'\n",
    "\n",
    "    plotVar_MergedPlatforms(merged_arr[var], var, title=title)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_arr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Dataframe of Filtered XARRAY\n",
    "This step is implemented to generate a CSV-structured dataframe, to then export to a CSV file expected by the RGeostats module.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create dataframe with filtered data and columns ['LATITUDE', 'LONGITUDE', 'TIME', 'TEMP', 'DEPTH']\n",
    "filtered2csv_multiDepths = pd.DataFrame() \n",
    "\n",
    "for pc in data_dict.keys():\n",
    "\n",
    "    for d in range(depth1, depth2+1):\n",
    "\n",
    "        # Create temporary dataframe\n",
    "        temp = pd.DataFrame()\n",
    "\n",
    "        data_depth_sel = data_dict[pc]['data'].isel(TIME=index_dict[pc],\n",
    "                                                    LATITUDE=index_dict[pc],\n",
    "                                                    LONGITUDE=index_dict[pc],\n",
    "                                                    DEPTH=slice(d, d+1))\n",
    "\n",
    "        for col in ['LONGITUDE', 'LATITUDE', 'TIME']:\n",
    "            temp[col.title()] = data_depth_sel[col].data.astype(float) \n",
    "\n",
    "        if 'TEMP' in vars_sel: temp['Temperature'] = data_depth_sel['TEMP'].data.astype(float) \n",
    "        if 'CNDC' in vars_sel: temp['Conductivity'] = data_depth_sel['CNDC'].data.astype(float) \n",
    "        if 'PSAL' in vars_sel: temp['Salinity'] = data_depth_sel['PSAL'].data.astype(float) \n",
    "\n",
    "        temp['Depth'] = d \n",
    "        temp['Vaissel_name'] = pc \n",
    "\n",
    "        filtered2csv_multiDepths = filtered2csv_multiDepths.append(temp, ignore_index=True)\n",
    "    \n",
    "# Rename index column with 'rank'\n",
    "filtered2csv_multiDepths = filtered2csv_multiDepths.rename_axis(\"rank\")\n",
    "\n",
    "display(filtered2csv_multiDepths)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Assign *Profil_id* to the unique positions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# find pair of unique coordinates \n",
    "unique_pos = filtered2csv_multiDepths.groupby(['Longitude','Latitude']).size().reset_index().rename(columns={0:'count'})\n",
    "\n",
    "prof_id = 1\n",
    "for long, lat in zip(unique_pos['Longitude'], unique_pos['Latitude']):\n",
    "    \n",
    "    # Define condition\n",
    "    cond = (filtered2csv_multiDepths['Longitude'] == long) & (filtered2csv_multiDepths['Latitude'] == lat)\n",
    "#     display(filtered2csv_multiDepths.loc[cond])\n",
    "    \n",
    "    # Assign unique Profil_id \n",
    "    filtered2csv_multiDepths.loc[cond,'Profil_id'] = prof_id\n",
    "    prof_id += 1\n",
    "\n",
    "# Convert to integer\n",
    "filtered2csv_multiDepths = filtered2csv_multiDepths.astype({'Profil_id': int})\n",
    "display(filtered2csv_multiDepths)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Export to CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save dataframe to csv\n",
    "data_dir = os.path.join(os.getcwd(), 'data_output')\n",
    "if not os.path.exists(data_dir): os.mkdir(data_dir)\n",
    "\n",
    "csvname = os.path.join(data_dir, \n",
    "                       f'filtered_BBOX={bbox_key}_MMYYYY={str(month).zfill(2)}{year}_\\\n",
    "                       Depth={depth1}-{depth2}m_VARS={\"_\".join(vars_sel)}.csv')\n",
    "\n",
    "filtered2csv_multiDepths.to_csv(csvname, sep=',', header=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Geostatistical Analysis with RGeostats\n",
    "## Set-up"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Install R Kernel & Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install rpy2\n",
    "\n",
    "%load_ext rpy2.ipython"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import variables from Python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "time_start = datetime(year, month1, day_start)\n",
    "time_end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bbox_key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "time_filter_str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import rpy2.robjects as robjects\n",
    "\n",
    "# BBOX\n",
    "# robjects.r(f'bbox_key = {bbox_key}')\n",
    "# robjects.r(f'bbox_values = {bbox_values}')\n",
    "\n",
    "# Time Range\n",
    "# robjects.r(f'time_start = {time_start}')\n",
    "# robjects.r(f'time_end = {time_end}')\n",
    "\n",
    "# Depth Range\n",
    "robjects.globalenv['depth'] = depth\n",
    "robjects.globalenv['depth1'] = depth1\n",
    "robjects.globalenv['depth2'] = depth2\n",
    "\n",
    "# robjects.r(f'depth = {depth}')\n",
    "# robjects.r(f'depth1 = {depth1}')\n",
    "# robjects.r(f'depth2 = {depth2}')\n",
    "\n",
    "# Mesh\n",
    "# robjects.r(f'mesh = {mesh}')\n",
    "robjects.globalenv['mesh'] = mesh\n",
    "\n",
    "# # CSV Name\n",
    "# robjects.r(f'csvname = {str(csvname)}')\n",
    "\n",
    "robjects.globalenv['csvname'] = csvname"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%R\n",
    "csvname"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Definition of the environment\n",
    "\n",
    "The next cells have specific contents for loading the library **RIntaros** that the user must choose to run or to skip. Their order is important."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "depth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%R \n",
    "suppressWarnings(suppressPackageStartupMessages(library(RIntaros)))\n",
    "\n",
    "# Defining if the data set must be read or not from the CSV (flag.read)\n",
    "flag.read = FALSE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading Data\n",
    "\n",
    "First of all, we setup some environment variables (data file name and bounding box).\n",
    "The **flag_file** allows the user to store each generated graphic file as a **PNG** file in the **image_name** directory, instead of plotting them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "attributes": {
     "": "",
     "classes": [],
     "id": ""
    }
   },
   "outputs": [],
   "source": [
    "%%R \n",
    "# Setup OUR environment and datasets \n",
    "dir.name   = getwd()\n",
    "data.name  = 'data_output'\n",
    "file.name  = 'filtered_AA_TEMP_CNDC_PSAL_0-50m.csv'\n",
    "long_lim   = c(-3,10)\n",
    "lat_lim    = c(56,60)\n",
    "\n",
    "intaros.save.environ(long_lim = long_lim, lat_lim = lat_lim,\n",
    "                     flag_file = FALSE)#, image_name = file.path(dir.name,image.name))\n",
    "\n",
    "depth1 = 0\n",
    "depth2 = 50\n",
    "var = \"Temperature\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we read the **CSV** file (taking the header line into account) and create the RGeostats Db. Finally we show the contents of the newly created Db (named **db0**)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%R \n",
    "if (flag.read || ! exists(\"db0\")) db0 = imr_read_csv(file.path(dir.name,data.name,file.name))\n",
    "db0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset global statistics\n",
    "\n",
    "We first establish the time amplitude of the dataset, as well as a set of colors assigned to each year."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "attributes": {
     "": "",
     "classes": [],
     "id": ""
    }
   },
   "outputs": [],
   "source": [
    "%%R\n",
    "years      = subyears = get_db_limits_year(db0)\n",
    "trimesters = subtrims = seq(1,4)\n",
    "colyears   = rg.colors(length(years))\n",
    "cat(build_title(\"The dataset period is:\",time2date(get_db_limits_time(db0))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us get some statistics on the information available"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%R\n",
    "db.stat.print(db0,funs=c(\"num\",\"mini\",\"maxi\",\"mean\"),\n",
    "              names=c(\"Longitude\",\"Latitude\",\"Depth\",\"Temperature\"))#,\"Conductivity\",\"Salinity\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Studying Temperature variable\n",
    "\n",
    "From this point, most of the calculations will be performed based on the **Temperature** variable.\n",
    "\n",
    "#### All Database\n",
    "\n",
    "We display all samples focusing on the variable in a 2D aerial view, reporting the country borders. For comparison, we define a common color scale, established on the global minimum and maximum values (**var_scale0**).\n",
    "Note that all samples from all years and all depths are displayed (slow operation)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "attributes": {
     "": "",
     "classes": [],
     "id": ""
    }
   },
   "outputs": [],
   "source": [
    "%%R\n",
    "colors.temp = rg.colors(rank=1)\n",
    "var_scale0 = get_db_limits_var(db0,var)\n",
    "display_var(db0, var = var, colors = colors.temp, title = var, filename = var)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Block Average at the given Depth\n",
    "\n",
    "The next display considers the variable averaged over the cells of a coarse grid (mesh of 1 degree)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "attributes": {
     "": "",
     "classes": [],
     "id": ""
    }
   },
   "outputs": [],
   "source": [
    "%%R\n",
    "var = \"Temperature\"\n",
    "\n",
    "# Select samples and set the color scale\n",
    "db1       = apply_sel(db0, depth_lim=c(depth1,depth2),compress = TRUE)\n",
    "var_scale = get_db_limits_var(db1,var)\n",
    "\n",
    "# Comment the following line if you want to to display all years\n",
    "subyears = years[1]\n",
    "\n",
    "date_lim  = create_limits_date(years[1])\n",
    "db        = apply_sel(db1, date_lim=date_lim)\n",
    "filename  = paste0(var,\"_Mean_\",years[1])\n",
    "title     = paste(\"Block Average for\", build_title(var, date_lim))\n",
    "dbg       = stats_grid(db, var, fun = \"mean\", mesh = 1)\n",
    "display_stats(dbg, var, var_scale = var_scale, colors = colors.temp,\n",
    "            title = title, filename = filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import in RGeostats database structure\n",
    "The new Db will be called **dbloc**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%R\n",
    "dbloc    = db0 #apply_sel(db0, long_lim = long_lim, lat_lim = lat_lim, compress = TRUE)\n",
    "# intaros.save.environ(long_lim = long_lim, lat_lim  = lat_lim)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Histogram of Measurement Depths\n",
    "\n",
    "We also aggregate the values of all samples vertically from 0m to 100m by 10 steps of 10m.\n",
    "Then, we can double-check this regularization step by plotting the histogram of the initial depths and the histogram of the depths in the aggregated file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%R\n",
    "# Aggregate data along Depth axis, from 5 to 10 by steps of 1\n",
    "# dbin:       input data grid\n",
    "# depth0:     origin of depth values\n",
    "# ddepth:     step along depth\n",
    "# ndepth:     number of depth steps\n",
    "\n",
    "# OUR 2003 dataset\n",
    "dbreg = dbloc #aggregate_depth(dbloc, depth0 = 0, ddepth = 10, ndepth = 10, flag.center = TRUE)\n",
    "\n",
    "\n",
    "# Histogram of depths\n",
    "hist(time2date(dbloc[,\"Time\"]),breaks=100,xlab=\"Time\",main=\"\")\n",
    "# hist(dbreg[,\"Time\"],breaks=100,xlab=\"Regular Depth\",main=\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Horizontal Variogram per Year\n",
    "\n",
    "Review the horizontal variograms for different years at 25m depth, calculated from the aggregated data set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%R\n",
    "# Define the active samples\n",
    "depth_lim = c(depth1,depth2)\n",
    "dbr       = apply_sel(dbreg, depth_lim = depth_lim, compress = TRUE)\n",
    "dbr       = db.locate(dbr,var,\"z\")\n",
    "\n",
    "# Variogram parameters\n",
    "vario_lag  = 0.5\n",
    "vario_nlag = 20\n",
    "varmax = 2\n",
    "\n",
    "# Loop on the years\n",
    "ecr = 1\n",
    "add = FALSE\n",
    "for (year in years) \n",
    "{\n",
    "  date_lim = create_limits_date(year)\n",
    "  dbr = remove_sel(dbr)\n",
    "  dbr = apply_sel(dbr, date_lim = date_lim)\n",
    "\n",
    "  vario = prepar_vario(dbr, dirvect=NA, \n",
    "                       vario_lag = vario_lag, vario_nlag = vario_nlag, draw.vario=TRUE,\n",
    "                       add=add, ylim=c(0,varmax), col=colyears[ecr], lwd=1,\n",
    "                       varline=FALSE, npairdw=TRUE)\n",
    "\n",
    "  ecr = ecr + 1\n",
    "  add = TRUE\n",
    "}\n",
    "legend(\"right\",legend=years,col=colyears,lty=1,lwd=2,cex=0.8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cross-Validation\n",
    "\n",
    "We first perform a cross-validation step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "attributes": {
     "": "",
     "classes": [],
     "id": ""
    }
   },
   "outputs": [],
   "source": [
    "%%R \n",
    "# Define the active samples\n",
    "depth_lim = c(depth1,depth2)\n",
    "date_lim  = create_limits_date(year, trimester=1) # NB change trimester number based on available dates\n",
    "dbr       = apply_sel(dbreg, depth_lim = depth_lim, date_lim = date_lim, compress = TRUE)\n",
    "\n",
    "dbreg\n",
    "\n",
    "# Variogram parameters\n",
    "vario_lag  = 0.5\n",
    "vario_nlag = 20\n",
    "\n",
    "# Perform the Cross-validation (includes Variogram calculation and Model fitting)\n",
    "dbp = xvalid_2D(dbr, var,  \n",
    "                vario_lag = vario_lag, vario_nlag = vario_nlag, struct = c(1,3,5,12), \n",
    "                dirvect = NA, draw.model=TRUE, radix=\"Xvalid\")\n",
    "db.stat.print(dbp,names=\"Xvalid*\",funs=c(\"num\",\"mean\",\"var\"),title=\"Cross-Validation Scores\")\n",
    "\n",
    "# Display the results\n",
    "filename  = paste0(\"Xvalid_\",var)\n",
    "display_var(dbp, var = \"*stderr\", flag.xvalid = TRUE,\n",
    "            title = paste(var, 'Cross-Validation Standard Error'), filename = filename, pos.legend=7)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2-D Estimation of Temperature\n",
    "\n",
    "We interpolate the Temperature for the given bounding box, at the given depth, and given time interval."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "attributes": {
     "": "",
     "classes": [],
     "id": ""
    }
   },
   "outputs": [],
   "source": [
    "%%R \n",
    "# Select the active samples\n",
    "depth = 10\n",
    "\n",
    "# Variogram parameters\n",
    "vario_lag  = 0.5\n",
    "vario_nlag = 20\n",
    "\n",
    "# Perform the interpolation (includes Variogram calculation and Model fitting)\n",
    "dbg = interpolate_2D(db0, var, mesh = 0.1, moving=FALSE,\n",
    "                     vario_lag = vario_lag, vario_nlag = vario_nlag, struct = c(1,3,5,12), \n",
    "                     dirvect = NA, draw.model=TRUE, pos.legend=1)\n",
    "\n",
    "# Display the results\n",
    "filename  = paste0(var,\".Estim2D_Year_\",year)\n",
    "display_result(db0, dbg, var = var, depth = depth, flag.estim = TRUE, \n",
    "               colors = colors.temp, filename = filename, pos.legend=7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:env_intaros]",
   "language": "python",
   "name": "conda-env-env_intaros-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
